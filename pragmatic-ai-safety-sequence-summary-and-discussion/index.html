<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon_package/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_package/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_package/favicon-16x16.png">
  <link rel="mask-icon" href="/images/favicon_package/safari-pinned-tab.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.artkpv.net","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"style":null,"show_result":false},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="This is summary and critique on two publications about AI Safety: Pragmatic AI Safety and Risks from Learned Optimization. They are an introductory and are aiming to direct research.">
<meta property="og:type" content="article">
<meta property="og:title" content="About PAIS and Risks from Learned Optimization">
<meta property="og:url" content="http://www.artkpv.net/pragmatic-ai-safety-sequence-summary-and-discussion/index.html">
<meta property="og:site_name" content="Artyom Karpov">
<meta property="og:description" content="This is summary and critique on two publications about AI Safety: Pragmatic AI Safety and Risks from Learned Optimization. They are an introductory and are aiming to direct research.">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-10-20T14:54:13.000Z">
<meta property="article:modified_time" content="2022-11-06T16:23:54.265Z">
<meta property="article:author" content="Artyom Karpov">
<meta property="article:tag" content="ml ai-safety">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://www.artkpv.net/pragmatic-ai-safety-sequence-summary-and-discussion/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://www.artkpv.net/pragmatic-ai-safety-sequence-summary-and-discussion/","path":"pragmatic-ai-safety-sequence-summary-and-discussion/","title":"About PAIS and Risks from Learned Optimization"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>About PAIS and Risks from Learned Optimization | Artyom Karpov</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-48410443-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-48410443-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="Artyom Karpov" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Artyom Karpov</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-user fa-fw"></i>Home</a></li><li class="menu-item menu-item-blog"><a href="/blog" rel="section"><i class="fa fa-pencil fa-fw"></i>Blog</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#The-Pragmatic-AI-Safety-sequence"><span class="nav-number">1.</span> <span class="nav-text">The Pragmatic AI Safety sequence</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Complex-Systems-for-AI-Safety-PAIS-3"><span class="nav-number">1.1.</span> <span class="nav-text">Complex Systems for AI Safety. PAIS 3</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Perform-Tractable-Research-While-Avoiding-Capabilities-Externalities-PAIS-4"><span class="nav-number">1.2.</span> <span class="nav-text">Perform Tractable Research While Avoiding Capabilities Externalities. PAIS 4</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Risks-from-Learned-Optimizaiton"><span class="nav-number">2.</span> <span class="nav-text">Risks from Learned Optimizaiton</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Artyom Karpov</p>
  <div class="site-description" itemprop="description">SE, ML, AI Safety, philosophy</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tag/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:artyomkarpov@gmail.com" title="E-Mail → mailto:artyomkarpov@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/artkpv" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;artkpv" rel="noopener me" target="_blank">GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://linkedin.com/in/artkpv" title="Linkedin → http:&#x2F;&#x2F;linkedin.com&#x2F;in&#x2F;artkpv" rel="noopener me" target="_blank">Linkedin</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://stackoverflow.com/users/511144" title="StackOverflow → http:&#x2F;&#x2F;stackoverflow.com&#x2F;users&#x2F;511144" rel="noopener me" target="_blank">StackOverflow</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/artkpv" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;artkpv" rel="noopener me" target="_blank"><i class="twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml" rel="noopener me"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://www.artkpv.net/pragmatic-ai-safety-sequence-summary-and-discussion/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Artyom Karpov">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Artyom Karpov">
      <meta itemprop="description" content="SE, ML, AI Safety, philosophy">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="About PAIS and Risks from Learned Optimization | Artyom Karpov">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          About PAIS and Risks from Learned Optimization
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-10-20 17:54:13" itemprop="dateCreated datePublished" datetime="2022-10-20T17:54:13+03:00">2022-10-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-11-06 19:23:54" itemprop="dateModified" datetime="2022-11-06T19:23:54+03:00">2022-11-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>This is summary and critique on two publications about AI Safety: Pragmatic AI Safety and Risks from Learned Optimization. They are an introductory and are aiming to direct research.</p>
<span id="more"></span>

<h1 id="The-Pragmatic-AI-Safety-sequence"><a href="#The-Pragmatic-AI-Safety-sequence" class="headerlink" title="The Pragmatic AI Safety sequence"></a>The Pragmatic AI Safety sequence</h1><p>Posted on EA forum: <a target="_blank" rel="noopener" href="https://forum.effectivealtruism.org/s/8EqNwueP6iw2BQpNo">https://forum.effectivealtruism.org/s/8EqNwueP6iw2BQpNo</a> .</p>
<h2 id="Complex-Systems-for-AI-Safety-PAIS-3"><a href="#Complex-Systems-for-AI-Safety-PAIS-3" class="headerlink" title="Complex Systems for AI Safety. PAIS 3"></a>Complex Systems for AI Safety. PAIS 3</h2><p>Article introduces complex systems theory and applies it to AI Safety field by naming factors that contribute to it. This addresses the third pillow in PAIS sequence (Systems view).</p>
<p>Claims:</p>
<ul>
<li>Complex systems (CS) found in AI Safety: neural nets, RL agent, AI lab, government and business, and other.</li>
<li>CSs have been well studied in Systems Theory (STAMP, cybernetics, causal models, other):<ul>
<li>CS: <ul>
<li>emergent properties not found in any of its parts;</li>
<li>highly interconnected (loops, indirect effects, many cause paths, interdependencies, etc.);</li>
<li>parts might be reliable, but the entire system is not;</li>
<li>modeling processes in terms of a control problem (process-controller).</li>
</ul>
</li>
<li>Risk management in a CS aims to handle factors that contribute to safety. Not only cause-effect chains (finding a root cause and fixing it). </li>
<li>Systems Theory is a superset of the chain of events model (finding a root cause), not a substitute.</li>
<li>The safety factors: safety culture (most important), cost of safety features, sound alarm systems, regulations and other.</li>
</ul>
</li>
<li>CS theories applied to AI Safety:<ul>
<li>Safety culture should be widespread through direct and indirect measures (from policy to everyday tools).</li>
<li>Researches and organizations should invest in safety.</li>
<li>We should address causes of safety neglectedness: company politics, profit incentives (need to improve cost-benefit for safety measures), philosophical (believing in technologies), x-risks, and other.</li>
<li>Other lessons from Systems Theory:<ul>
<li>A complex system develops its own goals (different from those given at the beginning) and pursues them. For AI: instrumental goals (not to be shut down).</li>
<li>We won’t be able to find where systems will fail or where its crucial parts by just observing its internal structure. </li>
<li>We should make simpler AI systems safe first, but that doesn’t guarantee safe larger AI systems.</li>
<li>We can’t rely on humans to make reliable systems as humans are unreliable.</li>
</ul>
</li>
<li>Overall research effort should be diversified while each researcher shouldn’t diversify their research widely (should be an expert in one area).</li>
</ul>
</li>
</ul>
<p>Pros:</p>
<ul>
<li>Takes top view on the field and shows its weak places to be addressed to make it safer.</li>
<li>Inherits complex systems theory which is based on decades of research and real-life cases and applies to ai safety. For example, Systems Theory stated that internal goals comes first long before it was found in AI systems (mesa-optimization).</li>
</ul>
<p>Discussion points:</p>
<ol>
<li>It is unclear how can we apply the safety culture in our current highly competitive environment (Google vs Facebook, China vs USA). What concretely should be the incentives or policies to adopt a safety culture? And who enforces them? If one adopts it, another will get a competitive advantage as they will spend more on capabilities and then ‘kill you’ (Yudkowsky, AGI Ruin).</li>
<li>Extremely high stakes, i.e. x-risk. While systems theory was developed for dangerous, mission-critical systems, it didn’t deal with those systems that might disempower all humanity forever. We don’t have a second try. So no use of systems theory? It should be an iterative process, but misaligned AI would kill us in a first wrong try?</li>
<li>Systems Theory developed for systems built by humans for humans. And humans have a certain limited intelligence level. Why is it true that it is likely that Systems Theory is applicable for a intelligence above human one?</li>
<li>Systems Theory implies the control of a better entity on a worse entity: government issues policies to control organizations, AI lab stops researches on a dangerous path, electrician complies with instructions, etc. Now, isn’t AGI a better entity to give control to? Does it imply the humanity’s dis-empowerment? Particularly, when we introduce a moral parliament (discussed in PAIS #4) won’t it mean that now this parliament is in power, not humanity?</li>
</ol>
<h2 id="Perform-Tractable-Research-While-Avoiding-Capabilities-Externalities-PAIS-4"><a href="#Perform-Tractable-Research-While-Avoiding-Capabilities-Externalities-PAIS-4" class="headerlink" title="Perform Tractable Research While Avoiding Capabilities Externalities. PAIS 4"></a>Perform Tractable Research While Avoiding Capabilities Externalities. PAIS 4</h2><p>How to perform long tail but safe AI research? This addresses the first (ML research precedents) and second (capabilities externalities) pillows in the PAIS sequence.</p>
<ul>
<li>Aim at long tail impact research (low probability, high stakes): no zero variables as long tail process multiplies rather than adds variables; research should be on tractable but unknown issues at the edge of known; start early; prepare for critical dangerous moments beforehand.</li>
<li>Research that operates near infinity variables or asymptotic (example is superintelligence) is problematic and should be avoided because they have longer feedback loops. Instead, aim for shorter feedback loops (low hanging fruits, best in terms of cost-benefit).</li>
<li>Discusses misinterpretations and misuses of Goodhart’s law. Mistake is to interpret it in a way that all measurements when became targets will inevitably result in failure to be a good measurement. Hence, all optimization systems will be misaligned as they will proxy objectives via some measurement (number of smiles). But it is more correct to say that those measurements <em>tend</em> to fail. And for AI Safety that means that we should be able to introduce a counteracting agent(s) for another AI agent(s) to align it.</li>
<li>As it is high stakes (humanity extinction), safety research should avoid capabilities advancement always where possible.</li>
</ul>
<h1 id="Risks-from-Learned-Optimizaiton"><a href="#Risks-from-Learned-Optimizaiton" class="headerlink" title="Risks from Learned Optimizaiton"></a>Risks from Learned Optimizaiton</h1><p><a target="_blank" rel="noopener" href="https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB">https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.01820">https://arxiv.org/abs/1906.01820</a></p>
<p>Parent optimization process might intentionally or unintentionally create another child optimization process. It’s more likely if a problem couldn’t be fully solved by a parent optimizer, there are too many policies to create; the smaller parent the more likelihood of child optimizer appearance and vice versa; the parent is biased towards simplicity. Less likely if the parent has less state or not enough power to find a child. Examples of child optimization are evolution and human brain, teacher and student, society and corporation, programmer and a program that searches solutions to NP problem, and so on.</p>
<p>A child is likely to have goals different to parent’s goals when it is free from parent’s control or deployed in another environment. In that case a child’s goals might be totally different from parent’s ones, or be near them, or be not optimal. If a child’s objective is totally different from the parent’s then it might nevertheless increase the parent’s objective if it doesn’t interfere with its own goals or if it can pursue its own goals afterwards. It is unclear how to match a child’s objective, which is indirectly learned, to the parent’s one (inner alignment). Especially risky is a child’s deception that happens if the child can project or plan many steps ahead, knows the parent’s objective, and there is a threat of shutting down or modification. Deception is more likely as we create objective proxies, or not train well before deployment.</p>
<p>It is likely that without child optimization systems will be much less useful but with this optimization in place the existential risk from AI increases.</p>
<p>Discussion points:</p>
<p>1.</p>
<p>In the glossary:</p>
<blockquote>
<p>An optimizer is a system that internally searches through some space…</p>
</blockquote>
<p>But shouldn’t it be a process and not a system? Shouldn’t it be only some computational process? To my mind the optimizer in this work is a computational process because it does a calculation to find an optimal solution. Still it is arguable if evolution is a computational process but I think it is in accordance with the Church-Turing thesis as optimization is a close ended and physical process. In that sense it is hard to imagine a pile of bricks or a rock to be an optimizer though they are systems.</p>
<p>2.</p>
<p>Would be good to have some predictions for claims to be falsifiable. As it is largely theoretical.</p>
<p>Example:</p>
<blockquote>
<p>At the same time, base optimizers are generally biased in favor of selecting learned algorithms with lower complexity. Thus, all else being equal, the base optimizer will generally be incentivized to look for a highly compressed policy.</p>
</blockquote>
<p>What experiment might prove it false or right? Also it seems that deception is a trickier algorithm than just following a base objective. So what experiment might find the bound where an algorithm should find such a more complex strategy as deception?</p>
<p>Also, it seems the paper misses a list of architectures (current or future) that might exhibit mesa-optimization. Or make a hypothesis of what architectures might produce mesa-optimization.</p>
<p>3.</p>
<p>I’m not sure that the mesa optimizer’s objective can’t be set directly. Why?</p>
<p>In</p>
<blockquote>
<p>Machine learning practitioners have direct control over the base objective function—either by specifying the loss function directly or training a model for it—but cannot directly specify the mesa-objective developed by a mesa-optimizer.</p>
</blockquote>
<p>Why can’t we make the base optimizer inject into mesa-optimizer’s objective some our objective? So that it is some mix of goals. This might be some high level restrictions like not to lie or not make some dangerous actions. That is, on each step when a base optimizer finishes with a current mesa-optimizer, why don’t we mix it with another model or strategy? Or why can’t we modify a final mesa-optimizer using reverse engineering and inject another strategy. Example might be a language model that has an output channel into a command line to execute commands and python programs from there. We can hijack its output channel and insert our python code.</p>
<p>4.</p>
<p>I agree it is right that it poses great risk when goals are misaligned. Because if we can’t control the mesa-objective then it is likely it would pursue some proxy objective like increasing the number of smiles or clicks. And, because of Goodhart’s law, there is a tendency that such goals fail to be a good objective. Canonical example of this is King Midas who wished to have gold from whatever he touched.</p>
<p>5.</p>
<p>Agree that it is more likely that mesa optimization will be present if problem space complexity increases as we see it in evolution. Still it is unclear how many iterations, and what time should pass before that moment. Evolution shows that complex systems appeared later than those that are simpler. So we might be satisfied with simple models and avoid complex ones. That is important because it is directly connected to x-risk and disempowerment of humanity by a more intelligent system(s).</p>
<p>6.</p>
<p>Agree that it is easier to find misaligned than an aligned optimizer because an aligned optimizer should be more complex. Simpler solutions are more reachable because adding features to the objective doesn’t make the optimizer find it quicker but usually harder. Example is a task to find a barber in some town and find a blond barber in this town. It might be the same probability if all barbers are blond but it is not possible that the probability will increase for the second case.</p>
<p>7.</p>
<p>Agree that the larger mesa-optimizer is the better aligned it is, but the less likely a base optimizer finds it. Because aligning powerful and critical optimizers should require a larger amount of information. Example is self-driving cars which might easily learn road signs but will require more space to learn how to react to various objects on the road (humans, dogs, etc.).</p>
<p>8.</p>
<p>Agree that deception is an especially hard problem, but it looks to me the paper misses additional discussion about model honesty or truthfulness which might be a solution to mesa-optimizer deception. Consider detecting deception at train time: if there exists a way to ask a mesa optimizer at train time if it will give different results at test / deploy time. In that case if we have an honest mesa optimizer that can somehow tell us this information then we detect that it will be a treacherous turn and modify its objection at the train time.</p>
<p>9.</p>
<p>It seems to me it needs more research about the risk from the collusion of deceptive agents or optimizers that might disempower humanity. Perhaps authors could propose an experiment that would prove the likelihood of simultaneous deception of several models.</p>
<p>10.</p>
<p>Regarding the conclusion. I’m not sure about the 3rd case in the consequences:</p>
<blockquote>
<p>Furthermore, in such a scenario, some parts of the outer alignment problem may not need to be solved either …</p>
</blockquote>
<p>Looks like we should solve the whole outer alignment in that case because what if a base optimizer nevertheless finds a way to mesa-optimization because it might have ability and we didn’t make it not to do so. Hence again we might face the inner alignment problem or even deception. It seems there is a small risk of it. Or we need more clarification, what does “preventing mesa-optimizers from arising’’ mean. Does it mean that it might happen occasionally but disabled, i.e. possibility of it still holds? I’m saying it because it is widely known how unstable and buggy our software currently is. There is even ‘a law’ for this, Murphy’s law, that says that what can be broken will be broken.</p>
<p>11.</p>
<p>It seems that the paper misses concrete examples of how mesa-optimization might occur in an optimizer that was not designed for mesa-optimization, i.e. unintended optimization. Here is an example derived from “The alignment problem from a deep learning perspective” by Richard Ngo. Suppose we can have one large optimizer that actively interacts with the world, some giant model constantly updating its weights and running in a large gpu farm. It will interact with the world by emitting actions on a given state, and the policy (mapping from states to actions) will be learned via self supervised learning (it is told or shown what next action should be given that input). So where is mesa-optimization here? How exactly does it map states to action? Is it like a transformer which is fine tuned for tasks? So this fine tuning will be like a mesa optimizer, i.e. it has this giant base optimizer that seems to be aligned with human values but when it does fine tuning for a specific task (say composing python programs) it creates a mesa-optimizer.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tag/ml-ai-safety/" rel="tag"># ml ai-safety</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/mlss-summer-2022/" rel="prev" title="ML Safety Scholars">
                  <i class="fa fa-angle-left"></i> ML Safety Scholars
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/Third-condition-of-the-deceptive-alignment/" rel="next" title="Third condition of the deceptive alignment">
                  Third condition of the deceptive alignment <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa-solid fa-brain-circuit"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Artyom Karpov</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
